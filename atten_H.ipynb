{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2gre9vSu0lIR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_inout_sequences_tra(data, tw, test_size):\n",
        "    \n",
        "    data = np.array(data)\n",
        "    L = len(data)\n",
        "    data = data[:(L//tw)*tw]\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(data[:-tw*test_size])\n",
        "    data = scaler.transform(data)\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(L//tw):\n",
        "        i = i*tw\n",
        "        train_seq = data[:,0:4][i:i+tw]\n",
        "        train_label = data[:,4][i:i+tw]\n",
        "        x.append(train_seq)\n",
        "        y.append(train_label)\n",
        "\n",
        "    y_train = torch.tensor(y[:-test_size])\n",
        "    y_test = torch.tensor(y[-test_size:])\n",
        "    x_train = torch.tensor(x[:-test_size])\n",
        "    x_test = torch.tensor(x[-test_size:])  \n",
        "    \n",
        "    train_dataset = data_utils.TensorDataset(x_train, y_train)\n",
        "    train_loader = data_utils.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "    test_dataset = data_utils.TensorDataset(x_test, y_test)\n",
        "    test_loader = data_utils.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "    \n",
        "    return train_loader, test_loader, scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-jcOezrw0YQK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_inout_sequences_opt(data, tw, test_size):\n",
        "    \n",
        "    data = np.array(data)\n",
        "    L = len(data)\n",
        "    data = data[:(L//tw)*tw]\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(data[:-tw*test_size])\n",
        "    data = scaler.transform(data)\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(L//tw):\n",
        "        i = i*tw\n",
        "        train_seq = data[:,0][i:i+tw]\n",
        "        train_label = data[:,1][i:i+tw]\n",
        "        x.append(train_seq)\n",
        "        y.append(train_label)\n",
        "\n",
        "    y_train = torch.tensor(y[:-test_size])\n",
        "    y_test = torch.tensor(y[-test_size:])\n",
        "    x_train = torch.tensor(x[:-test_size])\n",
        "    x_test = torch.tensor(x[-test_size:])  \n",
        "    \n",
        "    train_dataset = data_utils.TensorDataset(x_train, y_train)\n",
        "    train_loader = data_utils.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "    test_dataset = data_utils.TensorDataset(x_test, y_test)\n",
        "    test_loader = data_utils.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "    \n",
        "    return train_loader, test_loader, scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bIA0MnJt0du3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_inout_sequences_pm(data, tw, test_size):\n",
        "    \n",
        "    data = np.array(data)\n",
        "    L = len(data)\n",
        "    data = data[:(L//tw)*tw]\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(data[:-tw*test_size])\n",
        "    data = scaler.transform(data)\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(L//tw):\n",
        "        i = i*tw\n",
        "        train_seq = data[:,0:6][i:i+tw]\n",
        "        train_label = data[:,6][i:i+tw]\n",
        "        x.append(train_seq)\n",
        "        y.append(train_label)\n",
        "\n",
        "    y_train = torch.tensor(y[:-test_size])\n",
        "    y_test = torch.tensor(y[-test_size:])\n",
        "    x_train = torch.tensor(x[:-test_size])\n",
        "    x_test = torch.tensor(x[-test_size:])  \n",
        "    \n",
        "    train_dataset = data_utils.TensorDataset(x_train, y_train)\n",
        "    train_loader = data_utils.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "    test_dataset = data_utils.TensorDataset(x_test, y_test)\n",
        "    test_loader = data_utils.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "    \n",
        "    return train_loader, test_loader, scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rlrtfZtdzs0d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_inout_sequences_chick(data, tw, test_size):\n",
        "    \n",
        "    data = np.array(data)\n",
        "    L = len(data)\n",
        "    data = data[:(L//tw)*tw]\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(data[:-tw*test_size])\n",
        "    data = scaler.transform(data)\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(L//tw):\n",
        "        i = i*tw\n",
        "        train_seq = data[:,0:4][i:i+tw]\n",
        "        train_label = data[:,4][i:i+tw]\n",
        "        x.append(train_seq)\n",
        "        y.append(train_label)\n",
        "    \n",
        "    y_train = torch.tensor(y[:-test_size])\n",
        "    y_test = torch.tensor(y[-test_size:])\n",
        "    x_train = torch.tensor(x[:-test_size])\n",
        "    x_test = torch.tensor(x[-test_size:])  \n",
        "    # print(x_train[0])\n",
        "    # print(y_train[0])\n",
        "    train_dataset = data_utils.TensorDataset(x_train, y_train)\n",
        "    train_loader = data_utils.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "    test_dataset = data_utils.TensorDataset(x_test, y_test)\n",
        "    test_loader = data_utils.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "    \n",
        "    return train_loader, test_loader, scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vBA8E0m30noW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.distributions as distribution\n",
        "\n",
        "def init_network_weights(model):\n",
        "    if type(model) == torch.nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(model.weight)\n",
        "        if model.bias is not None:\n",
        "            model.bias.data.fill_(0.0)\n",
        "            \n",
        "def normal_distribution(mu, var):\n",
        "\n",
        "    #var = torch.exp(0.5*logvar)\n",
        "    normal = distribution.Normal(mu, var)\n",
        "    return normal\n",
        "\n",
        "\n",
        "def sample_from_normal(mu, var):\n",
        "    normal = normal_distribution(mu, var)\n",
        "    sample = normal.rsample()\n",
        "    return sample\n",
        "\n",
        "def loss_function(recon, data, Z_prior, Z_posterior, var = 1.0):\n",
        "\n",
        "    normal = distribution.Normal(recon, var)\n",
        "    log_likelihood = normal.log_prob(data).sum()\n",
        "\n",
        "    \n",
        "    prior = normal_distribution(Z_prior[0],Z_prior[1])\n",
        "    posterior = normal_distribution(Z_posterior[0],Z_posterior[1])\n",
        "    \n",
        "    #priorh = normal_distribution(torch.zeros_like(h_mean_var[0]),torch.ones_like(h_mean_var[1]))\n",
        "    #posteriorh = normal_distribution(h_mean_var[0],h_mean_var[1])\n",
        "    \n",
        "    KL = distribution.kl_divergence(posterior,prior).sum()\n",
        "    #KL3 = distribution.kl_divergence(posteriorh,priorh).sum()\n",
        "\n",
        "    return -log_likelihood + KL\n",
        "    #return -log_likelihood\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B0LIoeM104F_"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Jan 18 11:29:41 2021\n",
        "\n",
        "@author: tonyz\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def one_layer(input_size,output_size,activation,layer_type=None):\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        act = nn.ReLU(inplace=True)\n",
        "    if activation == \"tanh\":\n",
        "        act = nn.Tanh()\n",
        "    if activation == \"sigmoid\":\n",
        "        act = nn.Sigmoid()\n",
        "    layer=[]\n",
        "    if layer_type is None:\n",
        "        layer.append(nn.Linear(input_size,output_size))\n",
        "        layer.append(act)\n",
        "    else:\n",
        "        layer.append(nn.Linear(input_size,output_size))\n",
        "\n",
        "        \n",
        "    return nn.Sequential(*layer)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, layers, activation='relu'):\n",
        "\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.layers = layers\n",
        "        \n",
        "        sequence = []\n",
        "        for layer in range(self.layers-1):\n",
        "            if layer == 0:\n",
        "                sequence.append(one_layer(self.input_size,self.hidden_size,activation))\n",
        "            elif layer == self.layers-2:\n",
        "                sequence.append(one_layer(self.hidden_size,self.output_size,activation,\"output\"))\n",
        "            else:\n",
        "                sequence.append(one_layer(self.hidden_size,self.hidden_size,activation))\n",
        "        \n",
        "        self.mlp = nn.Sequential(*sequence)\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.mlp(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iJyUJM1wQ1fy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_x = torch.nn.Linear(input_size, hidden_size * 3, bias=bias)\n",
        "        self.lin_r = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.lin_z = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.lin_g = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        xr, xz, xg = torch.chunk(self.lin_x(x), 3, dim=-1)\n",
        "        r = torch.sigmoid(xr + self.lin_r(h))\n",
        "        z = torch.sigmoid(xz + self.lin_z(h))\n",
        "        g = torch.tanh(xg + self.lin_g(r * h))\n",
        "\n",
        "        h_new = z * h + (1-z) * g\n",
        "        dh = (1-z) * (g-h)\n",
        "        return h_new, dh\n",
        "\n",
        "class GRUVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, gru_hidden_size, inf_gru_hidden_size, output_size, Z_size, n_mlp, bias=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.input_size = input_size #size of gru\n",
        "        self.gru_hidden_size = gru_hidden_size #hidden size of gru-vae\n",
        "        self.inf_gru_hidden_size = inf_gru_hidden_size #hidden size of gru for Z inference\n",
        "        self.output_size = output_size\n",
        "        self.Z_size = Z_size #size of auxillary random var Z\n",
        "        self.n_mlp = n_mlp #number of mlp layers\n",
        "        \n",
        "        # Prior of Z\n",
        "        self.Z_prior = MLP(self.gru_hidden_size, self.gru_hidden_size, self.Z_size*2, self.n_mlp)\n",
        "        # Posterior of Z\n",
        "        \n",
        "\n",
        "        #self.gru_Z = nn.GRU(self.input_size, self.inf_gru_hidden_size, 1)\n",
        "        self.gru_Z = nn.GRU(1, self.inf_gru_hidden_size, 1)\n",
        "        self.Z_post = nn.Linear(self.inf_gru_hidden_size, self.Z_size * 2)\n",
        "        \n",
        "        # main gru model\n",
        "        self.gru = GRUCell(self.Z_size+self.input_size+self.gru_hidden_size, self.gru_hidden_size, bias=bias)\n",
        "        \n",
        "        # emission model\n",
        "        #self.emission = MLP(self.gru_hidden_size,self.gru_hidden_size,self.output_size*2,self.n_mlp)\n",
        "        self.emission = MLP(self.gru_hidden_size,self.gru_hidden_size,self.output_size,self.n_mlp)\n",
        "        # map hidden state to option price\n",
        "        self.map = nn.Linear(self.gru_hidden_size, self.output_size*2)\n",
        "        # self.mha = MultiHeadAttention(gru_hidden_size,4)\n",
        "\n",
        "        self.attension = nn.Linear(self.gru_hidden_size*2,1)\n",
        "        # self.adat = AdditiveAttention(self.gru_hidden_size,self.Z_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "    def Z_inference(self, gru_hidden_state):\n",
        "        Z_mean_var = self.Z_post(gru_hidden_state)\n",
        "        Z_mean_var = torch.chunk(Z_mean_var, 2, dim=-1)\n",
        "        Z_std = torch.exp(0.5*Z_mean_var[1]) # convert logvar to std\n",
        "        #Z_std = Z_mean_var[1]\n",
        "        Z = sample_from_normal(Z_mean_var[0],Z_std)\n",
        "        return Z, (Z_mean_var[0],Z_std)\n",
        "    \n",
        "    # returns next gru hidden state\n",
        "    def run_GRU(self, x_prime, gru_hidden_state):\n",
        "\n",
        "        gru_output,dh = self.gru(x_prime, gru_hidden_state)             \n",
        "        return gru_output,dh\n",
        "\n",
        "    def laten_att(self, encoder_output,hidden):\n",
        "        # print(input)\n",
        "        # print(encoder_output)\n",
        "        # print(encoder_output.shape)\n",
        "        # encoder_output = torch.FloatTensor(encoder_output).to(\"cuda\")\n",
        "        h_attension = hidden.repeat(encoder_output.size(0),1,1)\n",
        "        # print(h_attension.shape)\n",
        "        # print(encoder_output.shape)\n",
        "        attention = self.tanh(self.attension(torch.cat((h_attension,encoder_output),dim=2)))\n",
        "        attention = self.softmax(attention)\n",
        "        # print(attention)\n",
        "        c = torch.bmm(attention.permute(1,2,0),encoder_output.permute(1,0,2)).permute(1,0,2)\n",
        "        \n",
        "        return c[0][0],attention\n",
        "    \n",
        "    def reconstruct(self, gru_hidden_state):\n",
        "\n",
        "        reconstruction = self.emission(gru_hidden_state)\n",
        "        return (reconstruction,reconstruction,0)\n",
        "    \n",
        "\n",
        "    def run_GRUVAE(self, gru_hidden_size, inf_gru_hidden_size,x, y, h=False):\n",
        "\n",
        "        dhs = []\n",
        "        Zs, Z_posteriors, Z_priors = [], [], [] \n",
        "        y = y.view(len(y),1,1)\n",
        "        h_sequence = self.gru_Z(y.float())[0]\n",
        "        # y = y.view(1,1,len(y))\n",
        "        # h_sequence = self.CNN(y.float())\n",
        "        # print(h_sequence.shape)\n",
        "        if h is not False:\n",
        "            h=h\n",
        "        else:\n",
        "            h = torch.zeros(gru_hidden_size).to(\"cuda\")\n",
        "            Z_t = torch.zeros(self.Z_size).to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "            h_t = torch.zeros(self.gru_hidden_size).to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "            #h = h_sequence_reverse\n",
        "            #h = torch.randn(gru_hidden_size)\n",
        "        hidden_states = [h]\n",
        "        # print(Z_t)\n",
        "        for t in range(1,len(x)):\n",
        "            #x_prime = torch.cat([h,x[t].view(-1).float()])\n",
        "            Z_prior = self.Z_prior(h)   \n",
        "            Z_prior = torch.chunk(Z_prior, 2, dim=-1)\n",
        "            Z_std = torch.exp(0.5*Z_prior[1])\n",
        "            #Z_std = abs(Z_prior[1])\n",
        "            Z_priors.append((Z_prior[0],Z_std))\n",
        "            \n",
        "            Z, Z_posterior = self.Z_inference(h_sequence[t].view(inf_gru_hidden_size))\n",
        "            Z_posteriors.append(Z_posterior)\n",
        "            Zs.append(Z)\n",
        "            # Z_t = torch.cat((Z_t,Z.unsqueeze(0).unsqueeze(0)),0)\n",
        "            att,draw = self.laten_att(h_t,h)\n",
        "            # att,_ = self.adat(None,h.unsqueeze(0),Z_t[1:])\n",
        "            x_prime = torch.cat([att,Z,x[t].view(-1).float()])\n",
        "            h_new,dh = self.run_GRU(x_prime,h)\n",
        "            h_t = torch.cat((h_t,h_new.unsqueeze(0).unsqueeze(0)),0)\n",
        "            if t == 1:\n",
        "              h_t = h_t[1:]\n",
        "            hidden_states.append(h_new)\n",
        "            dhs.append(dh)\n",
        "            h = h_new\n",
        "        # tem =self.mha(Z_t[:,1:],Z_t[:,1:],h_t[:,1:],None)[0][0]\n",
        "        return Z_priors, Zs, Z_posteriors, hidden_states,dhs\n",
        "      \n",
        "    def forward(self, x, y, h=False):\n",
        "\n",
        "        Z_priors, Zs, Z_posteriors, hidden_states,dhs = self.run_GRUVAE(self.gru_hidden_size,self.inf_gru_hidden_size,x,y,h)\n",
        "        means = []\n",
        "        samples = []\n",
        "        for time in range(1,len(x)):\n",
        "            mean,sample,std = self.reconstruct(hidden_states[time])\n",
        "            means.append(mean)\n",
        "            samples.append(sample)\n",
        "\n",
        "        return means, samples, hidden_states, Z_priors, Zs, Z_posteriors, dhs, std\n",
        "        \n",
        "\n",
        "    \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")   \n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([1,2,3])[1:-1]"
      ],
      "metadata": {
        "id": "AHyjJycEgCzy",
        "outputId": "3a99ecb9-2656-4a10-e15e-53ded9818ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vB0Lgrtd2SHN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# from functions import loss_function\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class AutoEncoderTrainer:\n",
        "    \"\"\"AutoEncoder Training class.\"\"\"\n",
        "    def __init__(self, model, optimizer, train_loader, test_loader, validation_size): \n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        self.model = model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                                   \"cpu\")\n",
        "        self.optimizer = optimizer(self.model.parameters(), lr=0.001)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.validation_size = validation_size\n",
        "                \n",
        "        \n",
        "    def train_iter(self):\n",
        "        \"\"\"Single pass through the training data.\"\"\"\n",
        "        self.model.train()\n",
        "        x = self.train_loader.dataset.tensors[0].to(self.device)\n",
        "        y = self.train_loader.dataset.tensors[1].to(self.device)\n",
        "        loss = 0\n",
        "        self.optimizer.zero_grad()\n",
        "        for d in range(len(x)):\n",
        "            means,reconstructions, hidden_states, Z_priors, Zs, Z_posteriors, dhs,_ = self.model(x[d],y[d])\n",
        "            for i in range(len(reconstructions)):\n",
        "                L = loss_function(reconstructions[i], y[d][i+1].view(reconstructions[i].size()), Z_priors[i], Z_posteriors[i], var = 1.0)\n",
        "                loss += L\n",
        "        loss.backward()\n",
        "        self.optimizer.step() \n",
        "        copymodel = copy.deepcopy(self.model)\n",
        "        return loss, hidden_states, means, reconstructions, copymodel\n",
        "    \n",
        "    def val_iter(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_x = self.test_loader.dataset.tensors[0][:self.validation_size].to(self.device)\n",
        "            val_y = self.test_loader.dataset.tensors[1][:self.validation_size].to(self.device)\n",
        "            \n",
        "            metric = nn.MSELoss()\n",
        "            val_loss = 0\n",
        "            \n",
        "            for i in range(len(val_x)):\n",
        "                _, reconstructions, hidden_states, _, _, _, _, _ = self.model(val_x[i],val_y[i])\n",
        "                val_loss += metric(torch.stack(reconstructions).view(-1), val_y[i][1:]).detach().float()\n",
        "                \n",
        "        return val_loss \n",
        "            \n",
        "    def train_and_evaluate(self, epochs, params_select = False):\n",
        "        \"\"\"Run training and evaluation.\"\"\"\n",
        "        self.model.to(self.device)\n",
        "        model_list=[]\n",
        "        val_loss_list=[]\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, hidden_states,_,_,model = self.train_iter()\n",
        "            val_loss = self.val_iter()\n",
        "            model_list.append(model)\n",
        "            val_loss_list.append(val_loss)\n",
        "            \n",
        "            if not params_select:\n",
        "                print(f\"\\tEpoch: {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "                            \n",
        "        return model_list, val_loss_list\n",
        "\n",
        "            \n",
        "        \n",
        "        \n",
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))/torch.mean(torch.tensor(y))       \n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# os.chdir(\"C:/Users/tonyz/Desktop/GRUVAE/model\")\n",
        "# import gruvae\n",
        "# import train\n",
        "# from data_loader_chickenpox import create_inout_sequences\n",
        "# from functions import sample_from_normal\n",
        "\n",
        "\n",
        "# os.chdir(\"C:/Users/tonyz/Desktop/GRUVAE/data/usingmean\")\n",
        "\n",
        "data = pd.read_csv(\"chickenpox.csv\")\n",
        "data = data[[\"PEST\",\"BACS\",\"KOMAROM\",\"HEVES\",\"BUDAPEST\"]] \n",
        "\n",
        "train_steps = 300\n",
        "val_steps = 150\n",
        "\n",
        "\n",
        "tw = 10\n",
        "train_size = train_steps/tw\n",
        "validation_size = int(val_steps/tw)\n",
        "test_size = int(len(data)//tw-train_size)\n",
        "print(test_size)\n",
        "train_loader, test_loader,scaler = create_inout_sequences_chick(data, tw, test_size)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "gruVAE = GRUVAE(4,128,128,1,50,4)\n",
        "vae_trainer = AutoEncoderTrainer(gruVAE, optim.Adam, train_loader, test_loader, validation_size)\n",
        "model_list, val_loss_list = vae_trainer.train_and_evaluate(150)\n",
        "\n",
        "val, idx = min((val, idx) for (idx, val) in enumerate(val_loss_list))\n",
        "model = model_list[idx]\n",
        "\n",
        "\n",
        "\n",
        "def inverse(y, scaler):\n",
        "    d = torch.cat([torch.tensor(y),torch.tensor(y),\\\n",
        "                  torch.tensor(y),torch.tensor(y),torch.tensor(y)], dim=-1).numpy()\n",
        "    d = scaler.inverse_transform(d)\n",
        "    return d\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "prev_x = test_loader.dataset.tensors[0][:validation_size][-1].to(\"cuda\")\n",
        "prev_y = test_loader.dataset.tensors[1][:validation_size][-1].to(\"cuda\")\n",
        "print( test_loader.dataset.tensors[0][:validation_size].view(-1,4).shape)\n",
        "\n",
        "\n",
        "def compute_last_h(trials, prev_x, prev_y):\n",
        "    errors = []\n",
        "    h_list = []\n",
        "    metric = nn.MSELoss()\n",
        "    reconstruction_list = []\n",
        "    for i in range(trials):\n",
        "        _, reconstructions, hidden_states, _, _, _, _, _ = model(prev_x, prev_y)\n",
        "        errors.append(metric(torch.stack(reconstructions).view(-1)[-1], prev_y[1:][-1]))\n",
        "        h_list.append(hidden_states)\n",
        "        reconstruction_list.append(torch.stack(reconstructions).detach().to(\"cpu\").numpy())\n",
        "    val, idx = min((val, idx) for (idx, val) in enumerate(errors))\n",
        "    return h_list[idx][-1], reconstruction_list[idx]\n",
        "\n",
        "last_val_h, reconstructions = compute_last_h(500, prev_x, prev_y)\n",
        "\n",
        "\n",
        "pred_size = 50\n",
        "\n",
        "new_x = test_loader.dataset.tensors[0][validation_size:].view(-1,4).to(\"cuda\")\n",
        "new_y = test_loader.dataset.tensors[1][validation_size:].view(-1)\n",
        "new_y = inverse(new_y.view(-1,1),scaler)[:,4]\n",
        "print(new_x.shape)\n",
        "def make_predictions(model, pred_size, h_start, scaler, new_x):\n",
        "    predicted_prices = []\n",
        "    Z_t = torch.zeros(model.Z_size).to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "    h_t = h_start.to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "    hs = []\n",
        "    for i in range(pred_size):\n",
        "        new_Z = model.Z_prior(h_start)\n",
        "        new_Z = torch.chunk(new_Z, 2, dim=-1)\n",
        "        Z_std = torch.exp(0.5*new_Z[1])\n",
        "        new_Z = sample_from_normal(new_Z[0],Z_std)\n",
        "\n",
        "        att,draw = model.laten_att(h_t,h_start)\n",
        "        x_prime = torch.cat([att,new_Z,new_x[i].view(-1).float()])\n",
        "        h_start, ppp = model.run_GRU(x_prime,h_start)\n",
        "        h_t = torch.cat((h_t,h_start.unsqueeze(0).unsqueeze(0)),0)\n",
        "        hs.append(h_start)\n",
        "    for i in range(pred_size):\n",
        "        _,predicted_price,_ = model.reconstruct(hs[i])\n",
        "        predicted_price = predicted_price.to(\"cpu\")\n",
        "        predicted_price = inverse(predicted_price.view(-1,1),scaler)[:,4]\n",
        "        predicted_prices.append(max(predicted_price,0))\n",
        "    \n",
        "    return predicted_prices,draw\n",
        "    \n",
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))/torch.mean(torch.tensor(y))\n",
        "def trial(trials, model, hidden_states, pred_size, new_x):\n",
        "    predictions = []\n",
        "    upper_90 = []\n",
        "    lower_90 = []\n",
        "    upper_95 = []\n",
        "    lower_95 = []\n",
        "    mean = []\n",
        "    for i in range(trials):\n",
        "        predicted_prices,draw = make_predictions(model, pred_size, hidden_states, scaler, new_x)\n",
        "        predicted_prices = np.vstack(predicted_prices)\n",
        "        predictions.append(predicted_prices)\n",
        "       \n",
        "    predictions = np.concatenate(predictions, axis=1)\n",
        "        \n",
        "    for j in range(pred_size):\n",
        "        temp = predictions[j]\n",
        "        upper_90.append(np.percentile(temp,95))\n",
        "        lower_90.append(np.percentile(temp,5))\n",
        "        upper_95.append(np.percentile(temp,97.5))\n",
        "        lower_95.append(np.percentile(temp,2.5))\n",
        "        mean.append(np.mean(temp))\n",
        "    print(pred_size,RMSELoss(torch.tensor(mean),new_y[:pred_size]))\n",
        "    return upper_90, lower_90, upper_95, lower_95, mean,draw\n",
        "\n",
        "\n",
        "u_90, l_90, u_95, l_95, m,draw = trial(500, model, last_val_h, pred_size, new_x) #COMPUTE MEAN AND CONF INTERVALS\n",
        "\n",
        "###################################################################################\n",
        "#RUN THIS BLOCK TO PLOT GRAPH\n",
        "offset = 200\n",
        "plt.rcParams[\"figure.figsize\"] = (15,6)\n",
        "actual_values = data[\"BUDAPEST\"].tolist()[:tw*(len(data)//tw)]\n",
        "end_idx = val_steps+pred_size+train_steps\n",
        "start_idx = end_idx-pred_size-val_steps\n",
        "actual_range = actual_values[start_idx:end_idx]\n",
        "\n",
        "validation_end = len(actual_range) - pred_size + 1\n",
        "validation_start = validation_end - val_steps\n",
        "min_val = min(actual_range[validation_start:validation_end])\n",
        "max_val = max(actual_range[validation_start:validation_end])\n",
        "\n",
        "\n",
        "plt.plot(actual_range,color=\"teal\", label=\"actual\")\n",
        "plt.plot([i+val_steps for i in range(len(m))],m, color=\"deeppink\", label=\"mean prediction\")\n",
        "plt.fill_between([i+val_steps for i in range(len(m))], u_95, l_95,\n",
        "                 color='deeppink', alpha=0.2, label=\"95% confidence interval\")\n",
        "plt.fill_between([i+val_steps for i in range(len(m))], u_90, l_90,\n",
        "                 color='cyan', alpha=0.2, label=\"90% confidence interval\")\n",
        "\n",
        "plt.xlabel(\"observation point\")\n",
        "plt.ylabel(\"chickenpox cases\")\n",
        "\n",
        "plt.legend(loc=\"upper right\", prop={\"size\":15})\n",
        "plt.grid()\n",
        "ticks,labels = plt.xticks()\n",
        "ticks = np.array(list(ticks)[1:-1]).astype(int)\n",
        "labels = ticks+301\n",
        "plt.xticks(ticks, labels)\n",
        "# plt.savefig(\"atth.png\")\n",
        "pred_list = [5,10,15,20,25,30,40,50]\n",
        "for i in pred_list:\n",
        "  u_90, l_90, u_95, l_95, m,_= trial(500, model, last_val_h, i, new_x)"
      ],
      "metadata": {
        "id": "hl6szQse_xpx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xh8ASHSz1vcS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))/torch.mean(torch.tensor(y))\n",
        "\n",
        "data = pd.read_csv(\"AMZN_put.csv\").dropna()\n",
        "data = data[[\"Stock\",\"Option\"]] \n",
        "\n",
        "train_steps = 300\n",
        "val_steps = 30\n",
        "\n",
        "tw = 10\n",
        "train_size = train_steps/tw\n",
        "validation_size = int(val_steps/tw)\n",
        "test_size = int(len(data)//tw-train_size)\n",
        "\n",
        "train_loader, test_loader,scaler = create_inout_sequences_opt(data, tw, test_size)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "gruVAE = GRUVAE(1,64,64,1,50,4)\n",
        "vae_trainer = AutoEncoderTrainer(gruVAE, optim.Adam, train_loader, test_loader, validation_size)\n",
        "model_list, val_loss_list = vae_trainer.train_and_evaluate(300)\n",
        "\n",
        "val, idx = min((val, idx) for (idx, val) in enumerate(val_loss_list))\n",
        "model = model_list[idx]\n",
        "\n",
        "\n",
        "\n",
        "def inverse(y, scaler):\n",
        "    d = torch.cat([torch.tensor(y),torch.tensor(y)], dim=-1).numpy()\n",
        "    d = scaler.inverse_transform(d)\n",
        "    return d\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "prev_x = test_loader.dataset.tensors[0][:validation_size][-1].to(\"cuda\")\n",
        "prev_y = test_loader.dataset.tensors[1][:validation_size][-1].to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_last_h(trials, prev_x, prev_y):\n",
        "    errors = []\n",
        "    h_list = []\n",
        "    hs_list = []\n",
        "    metric = nn.MSELoss()\n",
        "    reconstruction_list = []\n",
        "    for i in range(trials):\n",
        "        _, reconstructions, hidden_states, _, _, _, _, _ = model(prev_x, prev_y)\n",
        "        errors.append(metric(torch.stack(reconstructions).view(-1)[-1], prev_y[1:][-1]))\n",
        "        h_list.append(hidden_states)\n",
        "        # hs_list.append(hs)\n",
        "\n",
        "        reconstruction_list.append(torch.stack(reconstructions).detach().to(\"cpu\").numpy())\n",
        "    val, idx = min((val, idx) for (idx, val) in enumerate(errors))\n",
        "    return h_list[idx][-1], reconstruction_list[idx]\n",
        "last_val_h, reconstructions = compute_last_h(500, prev_x, prev_y)\n",
        "\n",
        "\n",
        "pred_size = 30\n",
        "new_x = test_loader.dataset.tensors[0][validation_size:].view(-1,1).to(\"cuda\")\n",
        "new_y = test_loader.dataset.tensors[1][validation_size:].view(-1)\n",
        "new_y = inverse(new_y.view(-1,1),scaler)[:,1]\n",
        "\n",
        "def make_predictions(model, pred_size, h_start, scaler, new_x):\n",
        "    predicted_prices = []\n",
        "    Z_t = torch.zeros(model.Z_size).to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "    h_t = h_start.to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "    hs = []\n",
        "    for i in range(pred_size):\n",
        "        new_Z = model.Z_prior(h_start)\n",
        "        new_Z = torch.chunk(new_Z, 2, dim=-1)\n",
        "        Z_std = torch.exp(0.5*new_Z[1])\n",
        "        new_Z = sample_from_normal(new_Z[0],Z_std)\n",
        "        att,draw = model.laten_att(h_t,h_start)\n",
        "        x_prime = torch.cat([att,new_Z,new_x[i].view(-1).float()])\n",
        "        h_start,_ = model.run_GRU(x_prime,h_start)\n",
        "        h_t = torch.cat((h_t,h_start.unsqueeze(0).unsqueeze(0)),0)\n",
        "        hs.append(h_start)\n",
        "    for i in range(pred_size):\n",
        "        _,predicted_price,_ = model.reconstruct(hs[i])\n",
        "        predicted_price = predicted_price.to(\"cpu\")\n",
        "        predicted_price = inverse(predicted_price.view(-1,1),scaler)[:,1]\n",
        "        predicted_prices.append(max(predicted_price,0))\n",
        "    \n",
        "    return predicted_prices,draw\n",
        "def trial(trials, model, hidden_states, pred_size, new_x):\n",
        "    predictions = []\n",
        "    upper_90 = []\n",
        "    lower_90 = []\n",
        "    upper_95 = []\n",
        "    lower_95 = []\n",
        "    mean = []\n",
        "    for i in range(trials):\n",
        "        predicted_prices,_ = make_predictions(model, pred_size, hidden_states, scaler, new_x)\n",
        "        predicted_prices = np.vstack(predicted_prices)\n",
        "        predictions.append(predicted_prices)\n",
        "       \n",
        "    predictions = np.concatenate(predictions, axis=1)\n",
        "        \n",
        "    for j in range(pred_size):\n",
        "        temp = predictions[j]\n",
        "        upper_90.append(np.percentile(temp,95))\n",
        "        lower_90.append(np.percentile(temp,5))\n",
        "        upper_95.append(np.percentile(temp,97.5))\n",
        "        lower_95.append(np.percentile(temp,2.5))\n",
        "        mean.append(np.mean(temp))\n",
        "    print(pred_size,RMSELoss(torch.tensor(mean),new_y[:pred_size]))\n",
        "\n",
        "    return upper_90, lower_90, upper_95, lower_95, mean\n",
        "\n",
        "\n",
        "u_90, l_90, u_95, l_95, m = trial(500, model, last_val_h, pred_size, new_x)\n",
        "\n",
        "#############################################################################\n",
        "#run this block to plot graph\n",
        "offset = 200\n",
        "plt.rcParams[\"figure.figsize\"] = (15,6)\n",
        "actual_values = data[\"Option\"].tolist()[:tw*(len(data)//tw)]\n",
        "\n",
        "end_idx = train_steps+val_steps+pred_size\n",
        "actual_range = actual_values[:end_idx]\n",
        "\n",
        "validation_end = len(actual_range) - pred_size + 1\n",
        "validation_start = validation_end - val_steps\n",
        "min_val = min(actual_range[validation_start:validation_end])\n",
        "max_val = max(actual_range[validation_start:validation_end])\n",
        "\n",
        "\n",
        "plt.plot(actual_range,color=\"teal\", label=\"actual\")\n",
        "plt.plot([i+train_steps+val_steps for i in range(len(m))],m, color=\"deeppink\", label=\"mean prediction\")\n",
        "plt.fill_between([i+train_steps+val_steps for i in range(len(m))], u_95, l_95,\n",
        "                 color='deeppink', alpha=0.2, label=\"95% confidence interval\")\n",
        "plt.fill_between([i+train_steps+val_steps for i in range(len(m))], u_90, l_90,\n",
        "                 color='cyan', alpha=0.2, label=\"90% confidence interval\")\n",
        "\n",
        "plt.xlabel(\"Time/min\")\n",
        "plt.ylabel(\"Option Price\")\n",
        "plt.legend(loc=\"upper left\", prop={\"size\":15})\n",
        "plt.grid()\n",
        "ticks,labels = plt.xticks()\n",
        "ticks = np.array(list(ticks)[1:-1]).astype(int)\n",
        "labels = ticks+5\n",
        "plt.xticks(ticks, labels)\n",
        "\n",
        "# pred_list = [5,10,15,20,25,30]\n",
        "# for i in pred_list:\n",
        "#   u_90, l_90, u_95, l_95, m= trial(500, model, last_val_h, i, new_x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zUV-ueJq08gL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8ebZ51i08xU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SsBr-7Dl10Qj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"beijingpm.csv\")\n",
        "data = data[[\"Temperature\",\"Pressure\",\"WindSpeed\",\"DewPoint\",\"Rainfall\",\"Snow\",\"pm2.5\"]] \n",
        "\n",
        "\n",
        "train_steps = 1200\n",
        "val_steps = 200\n",
        "\n",
        "tw = 10\n",
        "train_size = train_steps/tw\n",
        "validation_size = int(val_steps/tw)\n",
        "test_size = int(len(data)//tw-train_size)\n",
        "\n",
        "\n",
        "train_loader, test_loader,scaler = create_inout_sequences_pm(data, tw, test_size)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "gruVAE = GRUVAE(6,64,64,1,50,4)\n",
        "vae_trainer = AutoEncoderTrainer(gruVAE, optim.Adam, train_loader, test_loader, validation_size)\n",
        "model_list, val_loss_list = vae_trainer.train_and_evaluate(300) \n",
        "\n",
        "\n",
        "val, idx = min((val, idx) for (idx, val) in enumerate(val_loss_list))\n",
        "model = model_list[idx]\n",
        "\n",
        "\n",
        "\n",
        "def inverse(y, scaler):\n",
        "    d = torch.cat([torch.tensor(y),torch.tensor(y),\\\n",
        "                  torch.tensor(y),torch.tensor(y),torch.tensor(y),torch.tensor(y),torch.tensor(y)], dim=-1).numpy()\n",
        "    d = scaler.inverse_transform(d)\n",
        "    return d\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "prev_x = test_loader.dataset.tensors[0][:validation_size][-1].to(\"cuda\")\n",
        "prev_y = test_loader.dataset.tensors[1][:validation_size][-1].to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "def compute_last_h(trials, prev_x, prev_y):\n",
        "    errors = []\n",
        "    h_list = []\n",
        "    metric = nn.MSELoss()\n",
        "    reconstruction_list = []\n",
        "    for i in range(trials):\n",
        "        # print(prev_x.shape)\n",
        "        _, reconstructions, hidden_states, _, _, _, _, _ = model(prev_x, prev_y)\n",
        "        errors.append(metric(torch.stack(reconstructions).view(-1)[-1], prev_y[1:][-1]))\n",
        "        h_list.append(hidden_states)\n",
        "        reconstruction_list.append(torch.stack(reconstructions).detach().to(\"cpu\").numpy())\n",
        "    val, idx = min((val, idx) for (idx, val) in enumerate(errors))\n",
        "    return h_list[idx][-1], reconstruction_list[idx]\n",
        "\n",
        "last_val_h, reconstructions = compute_last_h(500, prev_x, prev_y)\n",
        "\n",
        "\n",
        "pred_size = 30\n",
        "new_x = test_loader.dataset.tensors[0][validation_size:].view(-1,6).to(\"cuda\")\n",
        "new_y = test_loader.dataset.tensors[1][validation_size:].view(-1)\n",
        "new_y = inverse(new_y.view(-1,1),scaler)[:,6]\n",
        "\n",
        "\n",
        "def make_predictions(model, pred_size, h_start, scaler, new_x):\n",
        "    predicted_prices = []\n",
        "    Z_t = torch.zeros(model.Z_size).to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "    h_t = h_start.to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "    hs = []\n",
        "    for i in range(pred_size):\n",
        "        new_Z = model.Z_prior(h_start)\n",
        "        new_Z = torch.chunk(new_Z, 2, dim=-1)\n",
        "        Z_std = torch.exp(0.5*new_Z[1])\n",
        "        new_Z = sample_from_normal(new_Z[0],Z_std)\n",
        "\n",
        "        att,draw = model.laten_att(h_t,h_start)\n",
        "        x_prime = torch.cat([att,new_Z,new_x[i].view(-1).float()])\n",
        "        h_start,_ = model.run_GRU(x_prime,h_start)\n",
        "        h_t = torch.cat((h_t,h_start.unsqueeze(0).unsqueeze(0)),0)\n",
        "        hs.append(h_start)\n",
        "    for i in range(pred_size):\n",
        "        _,predicted_price,_ = model.reconstruct(hs[i])\n",
        "        predicted_price = predicted_price.to(\"cpu\")\n",
        "        predicted_price = inverse(predicted_price.view(-1,1),scaler)[:,6]\n",
        "        predicted_prices.append(max(predicted_price,0))\n",
        "    \n",
        "    return predicted_prices\n",
        "    \n",
        "\n",
        "def trial(trials, model, hidden_states, pred_size, new_x):\n",
        "    predictions = []\n",
        "    upper_90 = []\n",
        "    lower_90 = []\n",
        "    upper_95 = []\n",
        "    lower_95 = []\n",
        "    mean = []\n",
        "    for i in range(trials):\n",
        "        predicted_prices = make_predictions(model, pred_size, hidden_states, scaler, new_x)\n",
        "        predicted_prices = np.vstack(predicted_prices)\n",
        "        predictions.append(predicted_prices)\n",
        "       \n",
        "    predictions = np.concatenate(predictions, axis=1)\n",
        "        \n",
        "    for j in range(pred_size):\n",
        "        temp = predictions[j]\n",
        "        upper_90.append(np.percentile(temp,95))\n",
        "        lower_90.append(np.percentile(temp,5))\n",
        "        upper_95.append(np.percentile(temp,97.5))\n",
        "        lower_95.append(np.percentile(temp,2.5))\n",
        "        mean.append(np.mean(temp))\n",
        "    print(pred_size,RMSELoss(torch.tensor(mean),new_y[:pred_size]))\n",
        "\n",
        "    return upper_90, lower_90, upper_95, lower_95, mean\n",
        "\n",
        "\n",
        "u_90, l_90, u_95, l_95, m = trial(1000, model, last_val_h, pred_size, new_x)\n",
        "\n",
        "  \n",
        "############################################################################\n",
        "offset = 200\n",
        "plt.rcParams[\"figure.figsize\"] = (15,6)\n",
        "actual_values = data[\"pm2.5\"].tolist()[:tw*(len(data)//tw)]\n",
        "\n",
        "end_idx = val_steps+pred_size+train_steps\n",
        "start_idx = end_idx-pred_size-val_steps\n",
        "actual_range = actual_values[start_idx:end_idx]\n",
        "\n",
        "\n",
        "validation_end = len(actual_range) - pred_size + 1\n",
        "validation_start = validation_end - val_steps\n",
        "min_val = min(actual_range[validation_start:validation_end])\n",
        "max_val = max(actual_range[validation_start:validation_end])\n",
        "\n",
        "\n",
        "plt.plot(actual_range,color=\"teal\", label=\"actual\")\n",
        "plt.plot([i+val_steps for i in range(len(m))],m, color=\"deeppink\", label=\"mean prediction\")\n",
        "plt.fill_between([i+val_steps for i in range(len(m))], u_95, l_95,\n",
        "                 color='deeppink', alpha=0.2, label=\"95% confidence interval\")\n",
        "plt.fill_between([i+val_steps for i in range(len(m))], u_90, l_90,\n",
        "                 color='cyan', alpha=0.2, label=\"90% confidence interval\")\n",
        "\n",
        "plt.xlabel(\"observation point\")\n",
        "plt.ylabel(\"pm 2.5\")\n",
        "plt.legend(loc=\"upper left\",  prop={\"size\":15})\n",
        "plt.grid()\n",
        "ticks,labels = plt.xticks()\n",
        "ticks = np.array(list(ticks)[1:-1]).astype(int)\n",
        "labels = ticks+1201\n",
        "plt.xticks(ticks, labels)\n",
        "pred_list = [5,10,15,20,25,30,40,50]\n",
        "plt.savefig(\"attenh_pm_30.png\")\n",
        "for i in pred_list:\n",
        "  u_90, l_90, u_95, l_95, m = trial(500, model, last_val_h, i, new_x)\n",
        "#######################################\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z-iJBMhLJvRI"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}