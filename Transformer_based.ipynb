{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2gre9vSu0lIR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_inout_sequences_tra(data, tw, test_size):\n",
        "    \n",
        "    data = np.array(data)\n",
        "    L = len(data)\n",
        "    data = data[:(L//tw)*tw]\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(data[:-tw*test_size])\n",
        "    data = scaler.transform(data)\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(L//tw):\n",
        "        i = i*tw\n",
        "        train_seq = data[:,0:4][i:i+tw]\n",
        "        train_label = data[:,4][i:i+tw]\n",
        "        x.append(train_seq)\n",
        "        y.append(train_label)\n",
        "\n",
        "    y_train = torch.tensor(y[:-test_size])\n",
        "    y_test = torch.tensor(y[-test_size:])\n",
        "    x_train = torch.tensor(x[:-test_size])\n",
        "    x_test = torch.tensor(x[-test_size:])  \n",
        "    \n",
        "    train_dataset = data_utils.TensorDataset(x_train, y_train)\n",
        "    train_loader = data_utils.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "    test_dataset = data_utils.TensorDataset(x_test, y_test)\n",
        "    test_loader = data_utils.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "    \n",
        "    return train_loader, test_loader, scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-jcOezrw0YQK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def create_inout_sequences_opt(data, tw, test_size):\n",
        "    \n",
        "    data = np.array(data)\n",
        "    L = len(data)\n",
        "    data = data[:(L//tw)*tw]\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(data[:-tw*test_size])\n",
        "    data = scaler.transform(data)\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(L//tw):\n",
        "        i = i*tw\n",
        "        train_seq = data[:,0][i:i+tw]\n",
        "        train_label = data[:,1][i:i+tw]\n",
        "        x.append(train_seq)\n",
        "        y.append(train_label)\n",
        "\n",
        "    y_train = torch.tensor(y[:-test_size])\n",
        "    y_test = torch.tensor(y[-test_size:])\n",
        "    x_train = torch.tensor(x[:-test_size])\n",
        "    x_test = torch.tensor(x[-test_size:])  \n",
        "    \n",
        "    train_dataset = data_utils.TensorDataset(x_train, y_train)\n",
        "    train_loader = data_utils.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "    test_dataset = data_utils.TensorDataset(x_test, y_test)\n",
        "    test_loader = data_utils.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "    \n",
        "    return train_loader, test_loader, scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bIA0MnJt0du3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_inout_sequences_pm(data, tw, test_size):\n",
        "    \n",
        "    data = np.array(data)\n",
        "    L = len(data)\n",
        "    data = data[:(L//tw)*tw]\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(data[:-tw*test_size])\n",
        "    data = scaler.transform(data)\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(L//tw):\n",
        "        i = i*tw\n",
        "        train_seq = data[:,0:6][i:i+tw]\n",
        "        train_label = data[:,6][i:i+tw]\n",
        "        x.append(train_seq)\n",
        "        y.append(train_label)\n",
        "\n",
        "    y_train = torch.tensor(y[:-test_size])\n",
        "    y_test = torch.tensor(y[-test_size:])\n",
        "    x_train = torch.tensor(x[:-test_size])\n",
        "    x_test = torch.tensor(x[-test_size:])  \n",
        "    \n",
        "    train_dataset = data_utils.TensorDataset(x_train, y_train)\n",
        "    train_loader = data_utils.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "    test_dataset = data_utils.TensorDataset(x_test, y_test)\n",
        "    test_loader = data_utils.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "    \n",
        "    return train_loader, test_loader, scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rlrtfZtdzs0d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def create_inout_sequences_chick(data, tw, test_size):\n",
        "    \n",
        "    data = np.array(data)\n",
        "    L = len(data)\n",
        "    data = data[:(L//tw)*tw]\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(data[:-tw*test_size])\n",
        "    data = scaler.transform(data)\n",
        "    \n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(L//tw):\n",
        "        i = i*tw\n",
        "        train_seq = data[:,0:4][i:i+tw]\n",
        "        train_label = data[:,4][i:i+tw]\n",
        "        x.append(train_seq)\n",
        "        y.append(train_label)\n",
        "    \n",
        "    y_train = torch.tensor(y[:-test_size])\n",
        "    y_test = torch.tensor(y[-test_size:])\n",
        "    x_train = torch.tensor(x[:-test_size])\n",
        "    x_test = torch.tensor(x[-test_size:])  \n",
        "    # print(x_train[0])\n",
        "    # print(y_train[0])\n",
        "    train_dataset = data_utils.TensorDataset(x_train, y_train)\n",
        "    train_loader = data_utils.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
        "\n",
        "    test_dataset = data_utils.TensorDataset(x_test, y_test)\n",
        "    test_loader = data_utils.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "    \n",
        "    return train_loader, test_loader, scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vBA8E0m30noW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.distributions as distribution\n",
        "\n",
        "def init_network_weights(model):\n",
        "    if type(model) == torch.nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(model.weight)\n",
        "        if model.bias is not None:\n",
        "            model.bias.data.fill_(0.0)\n",
        "            \n",
        "def normal_distribution(mu, var):\n",
        "\n",
        "    #var = torch.exp(0.5*logvar)\n",
        "    normal = distribution.Normal(mu, var)\n",
        "    return normal\n",
        "\n",
        "\n",
        "def sample_from_normal(mu, var):\n",
        "    normal = normal_distribution(mu, var)\n",
        "    sample = normal.rsample()\n",
        "    return sample\n",
        "\n",
        "def loss_function(recon, data, Z_prior, Z_posterior, var = 1.0):\n",
        "\n",
        "    normal = distribution.Normal(recon, var)\n",
        "    log_likelihood = normal.log_prob(data).sum()\n",
        "\n",
        "    \n",
        "    prior = normal_distribution(Z_prior[0],Z_prior[1])\n",
        "    posterior = normal_distribution(Z_posterior[0],Z_posterior[1])\n",
        "    \n",
        "    #priorh = normal_distribution(torch.zeros_like(h_mean_var[0]),torch.ones_like(h_mean_var[1]))\n",
        "    #posteriorh = normal_distribution(h_mean_var[0],h_mean_var[1])\n",
        "    \n",
        "    KL = distribution.kl_divergence(posterior,prior).sum()\n",
        "    #KL3 = distribution.kl_divergence(posteriorh,priorh).sum()\n",
        "\n",
        "    return -log_likelihood + KL\n",
        "    #return -log_likelihood\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B0LIoeM104F_"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Jan 18 11:29:41 2021\n",
        "\n",
        "@author: tonyz\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def one_layer(input_size,output_size,activation,layer_type=None):\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        act = nn.ReLU(inplace=True)\n",
        "    if activation == \"tanh\":\n",
        "        act = nn.Tanh()\n",
        "    if activation == \"sigmoid\":\n",
        "        act = nn.Sigmoid()\n",
        "    layer=[]\n",
        "    if layer_type is None:\n",
        "        layer.append(nn.Linear(input_size,output_size))\n",
        "        layer.append(act)\n",
        "    else:\n",
        "        layer.append(nn.Linear(input_size,output_size))\n",
        "\n",
        "        \n",
        "    return nn.Sequential(*layer)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, layers, activation='relu'):\n",
        "\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.layers = layers\n",
        "        \n",
        "        sequence = []\n",
        "        for layer in range(self.layers-1):\n",
        "            if layer == 0:\n",
        "                sequence.append(one_layer(self.input_size,self.hidden_size,activation))\n",
        "            elif layer == self.layers-2:\n",
        "                sequence.append(one_layer(self.hidden_size,self.output_size,activation,\"output\"))\n",
        "            else:\n",
        "                sequence.append(one_layer(self.hidden_size,self.hidden_size,activation))\n",
        "        \n",
        "        self.mlp = nn.Sequential(*sequence)\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.mlp(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vrOH8nOYFDdA"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \n",
        "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))  \n",
        "\n",
        "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)  \n",
        "    scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # [..., seq_len_q, seq_len_k]\n",
        "\n",
        "    if mask is not None: \n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # [..., seq_len_q, seq_len_k]\n",
        "\n",
        "    output = torch.matmul(attention_weights, v)  # =>[..., seq_len_q, depth_v]\n",
        "    return output, attention_weights  # [..., seq_len_q, depth_v], [..., seq_le\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0  \n",
        "        self.depth = d_model // self.num_heads  \n",
        "\n",
        "        self.wq = torch.nn.Linear(d_model, d_model)\n",
        "        self.wk = torch.nn.Linear(d_model, d_model)\n",
        "        self.wv = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.final_linear = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):  # x [b, seq_len, d_model]\n",
        "        x = x.view(batch_size, -1, self.num_heads,\n",
        "                   self.depth)  #\n",
        "        return x.transpose(1, 2)  \n",
        "\n",
        "    def forward(self, q, k, v, mask):  \n",
        "        batch_size = q.shape[0]\n",
        "\n",
        "        q = self.wq(q)  # =>[b, seq_len, d_model]\n",
        "        k = self.wk(k)  # =>[b, seq_len, d_model]\n",
        "        v = self.wv(v)  # =>[b, seq_len, d_model]\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  \n",
        "        k = self.split_heads(k, batch_size)  \n",
        "        v = self.split_heads(v, batch_size)  \n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "       \n",
        "\n",
        "        scaled_attention = scaled_attention.transpose(1, 2)  \n",
        "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)  \n",
        "\n",
        "        output = self.final_linear(concat_attention)\n",
        "        return output, attention_weights  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OyD1h4I9vGe5"
      },
      "outputs": [],
      "source": [
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin_x = torch.nn.Linear(input_size, hidden_size * 3, bias=bias)\n",
        "        self.lin_r = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.lin_z = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.lin_g = torch.nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        xr, xz, xg = torch.chunk(self.lin_x(x), 3, dim=-1)\n",
        "        r = torch.sigmoid(xr + self.lin_r(h))\n",
        "        z = torch.sigmoid(xz + self.lin_z(h))\n",
        "        g = torch.tanh(xg + self.lin_g(r * h))\n",
        "\n",
        "        h_new = z * h + (1-z) * g\n",
        "        dh = (1-z) * (g-h)\n",
        "        return h_new, dh\n",
        "\n",
        "class GRUVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, gru_hidden_size, inf_gru_hidden_size, output_size, Z_size, n_mlp, head,need_emb = False,bias=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.input_size = input_size #size of gru\n",
        "        self.gru_hidden_size = gru_hidden_size #hidden size of gru-vae\n",
        "        self.inf_gru_hidden_size = inf_gru_hidden_size #hidden size of gru for Z inference\n",
        "        self.output_size = output_size\n",
        "        self.Z_size = Z_size #size of auxillary random var Z\n",
        "        self.n_mlp = n_mlp #number of mlp layers\n",
        "        \n",
        "        # Prior of Z\n",
        "        self.Z_prior = MLP(self.gru_hidden_size, self.gru_hidden_size, self.Z_size*2, self.n_mlp)\n",
        "        # Posterior of Z\n",
        "        #self.gru_Z = nn.GRU(self.input_size, self.inf_gru_hidden_size, 1)\n",
        "        self.gru_Z = nn.GRU(1, self.inf_gru_hidden_size, 1)\n",
        "        self.Z_post = nn.Linear(self.inf_gru_hidden_size, self.Z_size * 2)\n",
        "        \n",
        "        self.gru = GRUCell(self.Z_size+self.input_size, self.gru_hidden_size, bias=bias)\n",
        "        \n",
        "        # emission model\n",
        "        #self.emission = MLP(self.gru_hidden_size,self.gru_hidden_size,self.output_size*2,self.n_mlp)\n",
        "        self.emission = MLP(self.gru_hidden_size,self.gru_hidden_size,self.output_size,self.n_mlp)\n",
        "        \n",
        "        # map hidden state to option price\n",
        "        self.map = nn.Linear(self.gru_hidden_size, self.output_size*2)\n",
        "   \n",
        "        self.layernorm = torch.nn.LayerNorm(normalized_shape=self.gru_hidden_size , eps=1e-6)\n",
        "        self.mha = MultiHeadAttention(self.gru_hidden_size , head)\n",
        "        # self.mpp = MLP(self.Z_size+self.input_size,self.gru_hidden_size,self.gru_hidden_size,n_mlp1)\n",
        "        self.mpp = nn.Linear(self.Z_size+self.input_size, self.gru_hidden_size)\n",
        "        self.need_emb = need_emb\n",
        "        # self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    def Z_inference(self, gru_hidden_state):\n",
        "        Z_mean_var = self.Z_post(gru_hidden_state)\n",
        "        Z_mean_var = torch.chunk(Z_mean_var, 2, dim=-1)\n",
        "        Z_std = torch.exp(0.5*Z_mean_var[1]) # convert logvar to std\n",
        "        #Z_std = Z_mean_var[1]\n",
        "        Z = sample_from_normal(Z_mean_var[0],Z_std)\n",
        "        return Z, (Z_mean_var[0],Z_std)\n",
        "    \n",
        "    # returns next gru hidden state\n",
        "    def run_GRU(self, x_prime, gru_hidden_state):\n",
        "\n",
        "        gru_output,dh = self.gru(x_prime, gru_hidden_state)             \n",
        "        return gru_output,dh\n",
        "    \n",
        "    \n",
        "    def reconstruct(self, gru_hidden_state):\n",
        "\n",
        "        reconstruction = self.emission(gru_hidden_state)\n",
        "        #reconstruction = self.map(gru_hidden_state)\n",
        "        #reconstruction = torch.chunk(reconstruction, 2, dim=-1)\n",
        "        #mean = torch.exp(reconstruction[0])\n",
        "        #mean = reconstruction[0]\n",
        "        #reconstruction_std = torch.exp(0.5*reconstruction[1])\n",
        "        #reconstruction_std = reconstruction[1]\n",
        "        #reconstruct = sample_from_normal(mean,reconstruction_std)\n",
        "        return (reconstruction,reconstruction,0)\n",
        "    \n",
        "\n",
        "    def run_GRUVAE(self, gru_hidden_size, inf_gru_hidden_size,x, y, h=False):\n",
        "\n",
        "        dhs = []\n",
        "        Zs, Z_posteriors, Z_priors = [], [], [] \n",
        "        y = y.view(len(y),1,1)\n",
        "        h_sequence = self.gru_Z(y.float())[0]\n",
        "        # y = y.view(1,len(y),1)\n",
        "        # h_sequence = self.transformer_enc(y.float(),None)[0]\n",
        "        if h is not False:\n",
        "            h=h\n",
        "        else:\n",
        "            h = torch.zeros(gru_hidden_size).to(\"cuda\")\n",
        "\n",
        "        hidden_states = [h]\n",
        "        H_t = torch.zeros(gru_hidden_size).to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "        X_t = torch.zeros(self.Z_size+self.input_size).to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        for t in range(1,len(x)):\n",
        "            #x_prime = torch.cat([h,x[t].view(-1).float()])\n",
        "            Z_prior = self.Z_prior(h)   \n",
        "            Z_prior = torch.chunk(Z_prior, 2, dim=-1)\n",
        "            Z_std = torch.exp(0.5*Z_prior[1])\n",
        "            #Z_std = abs(Z_prior[1])\n",
        "            Z_priors.append((Z_prior[0],Z_std))\n",
        "            Z, Z_posterior = self.Z_inference(h_sequence[t].view(inf_gru_hidden_size))\n",
        "            Z_posteriors.append(Z_posterior)\n",
        "            Zs.append(Z)\n",
        "            x_prime = torch.cat([Z,x[t].view(-1).float()]).unsqueeze(0).unsqueeze(0)\n",
        "            # X_t = torch.cat((X_t,x_prime.unsqueeze(0).unsqueeze(0)),1)\n",
        "            if self.need_emb == True:\n",
        "              x_prime = self.mpp(x_prime)\n",
        "\n",
        "            tem = self.mha(x_prime,H_t,H_t,None)[0]\n",
        "            tem = self.layernorm(x_prime+tem)\n",
        "            H_t = torch.cat((H_t,tem),1)\n",
        "            if t==0:\n",
        "              H_t = H_t[:,1:]\n",
        "            h = tem[0][0]\n",
        "            hidden_states.append(h)\n",
        "            # print(tem.shape,H_t.shape)\n",
        "          \n",
        "        return Z_priors, Zs, Z_posteriors, hidden_states,dhs,None\n",
        "      \n",
        "    def forward(self, x, y, h=False):\n",
        "\n",
        "        Z_priors, Zs, Z_posteriors, hidden_states,dhs,enc = self.run_GRUVAE(self.gru_hidden_size,self.inf_gru_hidden_size,x,y,h)\n",
        "        means = []\n",
        "        samples = []\n",
        "        # print(hidden_states.shape)\n",
        "        for time in range(1,len(x)):\n",
        "            mean,sample,std = self.reconstruct(hidden_states[time])\n",
        "            means.append(mean)\n",
        "            samples.append(sample)\n",
        "\n",
        "        return means, samples, hidden_states, Z_priors, Zs, Z_posteriors, dhs, std,enc\n",
        "        \n",
        "\n",
        "    \n",
        "    \n",
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))/torch.mean(torch.tensor(y))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vB0Lgrtd2SHN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# from functions import loss_function\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class AutoEncoderTrainer:\n",
        "    \"\"\"AutoEncoder Training class.\"\"\"\n",
        "    def __init__(self, model, optimizer, train_loader, test_loader, validation_size,lr): \n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        self.model = model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                                   \"cpu\")\n",
        "        self.optimizer = optimizer(self.model.parameters(), lr=lr)\n",
        "        # self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        #     self.optimizer , 20,\n",
        "        #         gamma =0.9)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.validation_size = validation_size\n",
        "                \n",
        "        \n",
        "    def train_iter(self):\n",
        "        \"\"\"Single pass through the training data.\"\"\"\n",
        "        self.model.train()\n",
        "        x = self.train_loader.dataset.tensors[0].to(self.device)\n",
        "        y = self.train_loader.dataset.tensors[1].to(self.device)\n",
        "        loss = 0\n",
        "        self.optimizer.zero_grad()\n",
        "     \n",
        "        for d in range(len(x)):\n",
        "            # print(x[d].shape)÷\n",
        "            means,reconstructions, hidden_states, Z_priors, Zs, Z_posteriors,_,_,_= self.model(x[d],y[d])\n",
        "            for i in range(len(reconstructions)):\n",
        "                L = loss_function(reconstructions[i], y[d][i+1].view(reconstructions[i].size()), Z_priors[i], Z_posteriors[i], var = 1.0)\n",
        "                loss += L\n",
        "        loss.backward()\n",
        "        self.optimizer.step() \n",
        "        # self.scheduler.step()\n",
        "        copymodel = copy.deepcopy(self.model)\n",
        "        return loss, hidden_states, means, reconstructions, copymodel\n",
        "    \n",
        "    def val_iter(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_x = self.test_loader.dataset.tensors[0][:self.validation_size].to(self.device)\n",
        "            val_y = self.test_loader.dataset.tensors[1][:self.validation_size].to(self.device)\n",
        "            \n",
        "            metric = nn.MSELoss()\n",
        "            val_loss = 0\n",
        "           \n",
        "            for i in range(len(val_x)):\n",
        "                _, reconstructions, hidden_states, _, _, _, _,_,_ = self.model(val_x[i],val_y[i])\n",
        "                val_loss += metric(torch.stack(reconstructions).view(-1), val_y[i][1:]).detach().float()\n",
        "                \n",
        "        return val_loss \n",
        "            \n",
        "    def train_and_evaluate(self, epochs, params_select = False):\n",
        "        \"\"\"Run training and evaluation.\"\"\"\n",
        "        self.model.to(self.device)\n",
        "        model_list=[]\n",
        "        val_loss_list=[]\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, hidden_states,_,_,model = self.train_iter()\n",
        "            val_loss = self.val_iter()\n",
        "            model_list.append(model)\n",
        "            val_loss_list.append(val_loss)\n",
        "            \n",
        "            if not params_select:\n",
        "                print(f\"\\tEpoch: {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "                            \n",
        "        return model_list, val_loss_list\n",
        "\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")   \n",
        "    \n",
        "    \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "v9BmGVv2Jq-x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Yx8kY_Ps0tYH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"chickenpox.csv\")\n",
        "data = data[[\"PEST\",\"BACS\",\"KOMAROM\",\"HEVES\",\"BUDAPEST\"]] \n",
        "\n",
        "train_steps = 300\n",
        "val_steps = 150\n",
        "\n",
        "\n",
        "tw = 10\n",
        "train_size = train_steps/tw\n",
        "validation_size = int(val_steps/tw)\n",
        "test_size = int(len(data)//tw-train_size)\n",
        "\n",
        "train_loader, test_loader,scaler = create_inout_sequences_chick(data, tw, test_size)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "gruVAE = GRUVAE(4,128,128,1,124,4,4,False) \n",
        "\n",
        "vae_trainer = AutoEncoderTrainer(gruVAE, optim.Adam, train_loader, test_loader, validation_size,0.001)\n",
        "model_list, val_loss_list = vae_trainer.train_and_evaluate(150)\n",
        "\n",
        "val, idx = min((val, idx) for (idx, val) in enumerate(val_loss_list))\n",
        "model = model_list[idx]\n",
        "\n",
        "\n",
        "\n",
        "def inverse(y, scaler):\n",
        "    d = torch.cat([torch.tensor(y),torch.tensor(y),\\\n",
        "                  torch.tensor(y),torch.tensor(y),torch.tensor(y)], dim=-1).numpy()\n",
        "    d = scaler.inverse_transform(d)\n",
        "    return d\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "prev_x = test_loader.dataset.tensors[0][:validation_size][-1].to(\"cuda\")\n",
        "prev_y = test_loader.dataset.tensors[1][:validation_size][-1].to(\"cuda\")\n",
        "\n",
        "\n",
        "def compute_last_h(trials, prev_x, prev_y):\n",
        "    errors = []\n",
        "    h_list = []\n",
        " \n",
        "    metric = nn.MSELoss()\n",
        "    reconstruction_list = []\n",
        "    enc_list = []\n",
        "    for i in range(trials):\n",
        "        _, reconstructions, hidden_states, _, _,_,_,_,enc = model(prev_x, prev_y)\n",
        "        errors.append(metric(torch.stack(reconstructions).view(-1)[-1], prev_y[1:][-1]))\n",
        "        h_list.append(hidden_states)\n",
        "        enc_list.append(enc)\n",
        "        reconstruction_list.append(torch.stack(reconstructions).detach().to(\"cpu\").numpy())\n",
        "    val, idx = min((val, idx) for (idx, val) in enumerate(errors))\n",
        "    return h_list[idx][-1], reconstruction_list[idx],enc_list[idx]\n",
        "\n",
        "last_val_h, reconstructions,enc_list = compute_last_h(500, prev_x, prev_y)\n",
        "\n",
        "# print(last_val_h.shape)\n",
        "pred_size = 30\n",
        "new_x = test_loader.dataset.tensors[0][validation_size:].view(-1,4).to(\"cuda\")\n",
        "new_y = test_loader.dataset.tensors[1][validation_size:].view(-1)\n",
        "new_y = inverse(new_y.view(-1,1),scaler)[:,4]\n",
        "# print(new_x.shape)\n",
        "def make_predictions(model, pred_size, h_start, scaler, new_x,enc_list):\n",
        "    predicted_prices = []\n",
        "    H_t = h_start.to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    \n",
        "    for i in range(pred_size):\n",
        "\n",
        "        new_Z = model.Z_prior(h_start)\n",
        "        new_Z = torch.chunk(new_Z, 2, dim=-1)\n",
        "        Z_std = torch.exp(0.5*new_Z[1])\n",
        "        new_Z = sample_from_normal(new_Z[0],Z_std)\n",
        "        # print(new_Z\n",
        "        x_prime = torch.cat([new_Z, new_x[i].view(-1).float()]).unsqueeze(0).unsqueeze(0)\n",
        "        if model.need_emb == True:\n",
        "            x_prime = model.mpp(x_prime)\n",
        "        tem = model.mha(x_prime,H_t,H_t,None)[0]\n",
        "        tem = model.layernorm(x_prime+tem)\n",
        "        # tem = model.ffn(tem)\n",
        "        H_t = torch.cat((H_t,tem),1)\n",
        "        h_start = tem[0][0]\n",
        "        _,predicted_price,_ = model.reconstruct(h_start)\n",
        "        predicted_price = predicted_price.to(\"cpu\")\n",
        "        predicted_price = inverse(predicted_price.view(-1,1),scaler)[:,4]\n",
        "        predicted_prices.append(max(predicted_price,0))\n",
        "\n",
        "    \n",
        "    # for i in range(pred_size):\n",
        "        \n",
        "    return predicted_prices\n",
        "    \n",
        "    \n",
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))/torch.mean(torch.tensor(y))\n",
        "\n",
        "def trial(trials, model, hidden_states, pred_size, new_x,enc_list):\n",
        "    predictions = []\n",
        "    upper_90 = []\n",
        "    lower_90 = []\n",
        "    upper_95 = []\n",
        "    lower_95 = []\n",
        "    mean = []\n",
        "    for i in range(trials):\n",
        "        predicted_prices = make_predictions(model, pred_size, hidden_states, scaler, new_x,enc_list)\n",
        "        predicted_prices = np.vstack(predicted_prices)\n",
        "        predictions.append(predicted_prices)\n",
        "       \n",
        "    predictions = np.concatenate(predictions, axis=1)\n",
        "        \n",
        "    for j in range(pred_size):\n",
        "        temp = predictions[j]\n",
        "        upper_90.append(np.percentile(temp,95))\n",
        "        lower_90.append(np.percentile(temp,5))\n",
        "        upper_95.append(np.percentile(temp,97.5))\n",
        "        lower_95.append(np.percentile(temp,2.5))\n",
        "        mean.append(np.mean(temp))\n",
        "    print(pred_size,RMSELoss(torch.tensor(mean),new_y[:pred_size]))\n",
        "    return upper_90, lower_90, upper_95, lower_95, mean\n",
        "\n",
        "\n",
        "u_90, l_90, u_95, l_95, m = trial(500, model, last_val_h, pred_size, new_x,enc_list) #COMPUTE MEAN AND CONF INTERVALS\n",
        "\n",
        "###################################################################################\n",
        "#RUN THIS BLOCK TO PLOT GRAPH\n",
        "offset = 200\n",
        "plt.rcParams[\"figure.figsize\"] = (15,6)\n",
        "actual_values = data[\"BUDAPEST\"].tolist()[:tw*(len(data)//tw)]\n",
        "end_idx = val_steps+pred_size+train_steps\n",
        "start_idx = end_idx-pred_size-val_steps\n",
        "actual_range = actual_values[start_idx:end_idx]\n",
        "\n",
        "validation_end = len(actual_range) - pred_size + 1\n",
        "validation_start = validation_end - val_steps\n",
        "min_val = min(actual_range[validation_start:validation_end])\n",
        "max_val = max(actual_range[validation_start:validation_end])\n",
        "\n",
        "\n",
        "plt.plot(actual_range,color=\"teal\", label=\"actual\")\n",
        "plt.plot([i+val_steps for i in range(len(m))],m, color=\"deeppink\", label=\"mean prediction\")\n",
        "plt.fill_between([i+val_steps for i in range(len(m))], u_95, l_95,\n",
        "                 color='deeppink', alpha=0.2, label=\"95% confidence interval\")\n",
        "plt.fill_between([i+val_steps for i in range(len(m))], u_90, l_90,\n",
        "                 color='cyan', alpha=0.2, label=\"90% confidence interval\")\n",
        "\n",
        "plt.xlabel(\"observation point\")\n",
        "plt.ylabel(\"chickenpox cases\")\n",
        "\n",
        "plt.legend(loc=\"upper right\", prop={\"size\":15})\n",
        "plt.grid()\n",
        "ticks,labels = plt.xticks()\n",
        "ticks = np.array(list(ticks)[1:-1]).astype(int)\n",
        "labels = ticks+301\n",
        "plt.xticks(ticks, labels)\n",
        "plt.savefig(\"trans50.png\")\n",
        "pred_list = [5,10,15,20,25,30,40,50]\n",
        "for i in pred_list:\n",
        "  u_90, l_90, u_95, l_95, m = trial(500, model, last_val_h, i, new_x,None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xh8ASHSz1vcS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "data = pd.read_csv(\"AMZN_put.csv\").dropna()\n",
        "data = data[[\"Stock\",\"Option\"]] \n",
        "\n",
        "train_steps = 300\n",
        "val_steps = 30\n",
        "\n",
        "tw = 10\n",
        "train_size = train_steps/tw\n",
        "validation_size = int(val_steps/tw)\n",
        "test_size = int(len(data)//tw-train_size)\n",
        "\n",
        "train_loader, test_loader,scaler = create_inout_sequences_opt(data, tw, test_size)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "gruVAE = GRUVAE(1,64,64,1,50,4,4,True)\n",
        "vae_trainer = AutoEncoderTrainer(gruVAE, optim.Adam, train_loader, test_loader, validation_size,0.0009)\n",
        "model_list, val_loss_list = vae_trainer.train_and_evaluate(300)\n",
        "\n",
        "val, idx = min((val, idx) for (idx, val) in enumerate(val_loss_list))\n",
        "model = model_list[idx]\n",
        "\n",
        "\n",
        "\n",
        "def inverse(y, scaler):\n",
        "    d = torch.cat([torch.tensor(y),torch.tensor(y)], dim=-1).numpy()\n",
        "    d = scaler.inverse_transform(d)\n",
        "    return d\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "prev_x = test_loader.dataset.tensors[0][:validation_size][-1].to(\"cuda\")\n",
        "prev_y = test_loader.dataset.tensors[1][:validation_size][-1].to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "def compute_last_h(trials, prev_x, prev_y):\n",
        "    errors = []\n",
        "    h_list = []\n",
        "    hs_list = []\n",
        "    metric = nn.MSELoss()\n",
        "    reconstruction_list = []\n",
        "    for i in range(trials):\n",
        "        _, reconstructions, hidden_states, _, _, _, _, _,hs = model(prev_x, prev_y)\n",
        "        errors.append(metric(torch.stack(reconstructions).view(-1)[-1], prev_y[1:][-1]))\n",
        "        h_list.append(hidden_states)\n",
        "        hs_list.append(hs)\n",
        "\n",
        "        reconstruction_list.append(torch.stack(reconstructions).detach().to(\"cpu\").numpy())\n",
        "    val, idx = min((val, idx) for (idx, val) in enumerate(errors))\n",
        "    return h_list[idx][-1], reconstruction_list[idx]\n",
        "\n",
        "last_val_h, reconstructions = compute_last_h(500, prev_x, prev_y)\n",
        "\n",
        "\n",
        "pred_size = 30\n",
        "new_x = test_loader.dataset.tensors[0][validation_size:].view(-1,1).to(\"cuda\")\n",
        "new_y = test_loader.dataset.tensors[1][validation_size:].view(-1)\n",
        "new_y = inverse(new_y.view(-1,1),scaler)[:,1]\n",
        "\n",
        "def make_predictions(model, pred_size, h_start, scaler, new_x,enc_list):\n",
        "    predicted_prices = []\n",
        "    # tem_hs = [h_start.tolist()]\n",
        "    # tem_h = torch.zeros(128)\n",
        "    # h_start = tem_h.to(\"cuda\")\n",
        "    # tem_hs = hs.unsqueeze(0).to(\"cuda\")\n",
        "    # enc_out = enc_list\n",
        "\n",
        "    # enc_out = torch.cat((enc_out,x_prime.unsqueeze(0).unsqueeze(0)),1)\n",
        "    H_t = h_start.to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    \n",
        "    for i in range(pred_size):\n",
        "\n",
        "        new_Z = model.Z_prior(h_start)\n",
        "        new_Z = torch.chunk(new_Z, 2, dim=-1)\n",
        "        Z_std = torch.exp(0.5*new_Z[1])\n",
        "        new_Z = sample_from_normal(new_Z[0],Z_std)\n",
        "        # print(new_Z\n",
        "        x_prime = torch.cat([new_Z, new_x[i].view(-1).float()]).unsqueeze(0).unsqueeze(0)\n",
        "        if model.need_emb == True:\n",
        "            x_prime = model.mpp(x_prime)\n",
        "        tem = model.mha(x_prime,H_t,H_t,None)[0]\n",
        "        tem = model.layernorm(x_prime+tem)\n",
        "        # tem = model.ffn(tem)\n",
        "        H_t = torch.cat((H_t,tem),1)\n",
        "        h_start = tem[0][0]\n",
        "        _,predicted_price,_ = model.reconstruct(h_start)\n",
        "        predicted_price = predicted_price.to(\"cpu\")\n",
        "        predicted_price = inverse(predicted_price.view(-1,1),scaler)[:,1]\n",
        "        predicted_prices.append(max(predicted_price,0))\n",
        "\n",
        "    \n",
        "    # for i in range(pred_size):\n",
        "        \n",
        "    return predicted_prices\n",
        "    \n",
        "\n",
        "def trial(trials, model, hidden_states, pred_size, new_x):\n",
        "    predictions = []\n",
        "    upper_90 = []\n",
        "    lower_90 = []\n",
        "    upper_95 = []\n",
        "    lower_95 = []\n",
        "    mean = []\n",
        "    for i in range(trials):\n",
        "        predicted_prices = make_predictions(model, pred_size, hidden_states, scaler, new_x,None)\n",
        "        predicted_prices = np.vstack(predicted_prices)\n",
        "        predictions.append(predicted_prices)\n",
        "       \n",
        "    predictions = np.concatenate(predictions, axis=1)\n",
        "        \n",
        "    for j in range(pred_size):\n",
        "        temp = predictions[j]\n",
        "        upper_90.append(np.percentile(temp,95))\n",
        "        lower_90.append(np.percentile(temp,5))\n",
        "        upper_95.append(np.percentile(temp,97.5))\n",
        "        lower_95.append(np.percentile(temp,2.5))\n",
        "        mean.append(np.mean(temp))\n",
        "    print(pred_size,RMSELoss(torch.tensor(mean),new_y[:pred_size]))\n",
        "\n",
        "    return upper_90, lower_90, upper_95, lower_95, mean\n",
        "\n",
        "\n",
        "u_90, l_90, u_95, l_95, m = trial(500, model, last_val_h, pred_size, new_x)\n",
        "\n",
        "#############################################################################\n",
        "#run this block to plot graph\n",
        "offset = 200\n",
        "plt.rcParams[\"figure.figsize\"] = (15,6)\n",
        "actual_values = data[\"Option\"].tolist()[:tw*(len(data)//tw)]\n",
        "\n",
        "end_idx = train_steps+val_steps+pred_size\n",
        "actual_range = actual_values[:end_idx]\n",
        "\n",
        "validation_end = len(actual_range) - pred_size + 1\n",
        "validation_start = validation_end - val_steps\n",
        "min_val = min(actual_range[validation_start:validation_end])\n",
        "max_val = max(actual_range[validation_start:validation_end])\n",
        "\n",
        "\n",
        "plt.plot(actual_range,color=\"teal\", label=\"actual\")\n",
        "plt.plot([i+train_steps+val_steps for i in range(len(m))],m, color=\"deeppink\", label=\"mean prediction\")\n",
        "plt.fill_between([i+train_steps+val_steps for i in range(len(m))], u_95, l_95,\n",
        "                 color='deeppink', alpha=0.2, label=\"95% confidence interval\")\n",
        "plt.fill_between([i+train_steps+val_steps for i in range(len(m))], u_90, l_90,\n",
        "                 color='cyan', alpha=0.2, label=\"90% confidence interval\")\n",
        "\n",
        "plt.xlabel(\"Time/min\")\n",
        "plt.ylabel(\"Option Price\")\n",
        "plt.legend(loc=\"upper left\", prop={\"size\":15})\n",
        "plt.grid()\n",
        "ticks,labels = plt.xticks()\n",
        "ticks = np.array(list(ticks)[1:-1]).astype(int)\n",
        "labels = ticks+5\n",
        "plt.xticks(ticks, labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RyNkVKkIyAmR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# os.chdir(\"C:/Users/tonyz/Desktop/GRUVAE/model\")\n",
        "# import gruvae\n",
        "# import train\n",
        "# # from data_loader_pm import create_inout_sequences\n",
        "# # from functions import sample_from_normal\n",
        "\n",
        "\n",
        "# # # # os.chdir(\"C:/Users/tonyz/Desktop/GRUVAE/data/usingmean\")\n",
        "\n",
        "data = pd.read_csv(\"beijingpm.csv\")\n",
        "data = data[[\"Temperature\",\"Pressure\",\"WindSpeed\",\"DewPoint\",\"Rainfall\",\"Snow\",\"pm2.5\"]] \n",
        "\n",
        "\n",
        "train_steps = 1200\n",
        "val_steps = 200\n",
        "\n",
        "tw = 10\n",
        "train_size = train_steps/tw\n",
        "validation_size = int(val_steps/tw)\n",
        "test_size = int(len(data)//tw-train_size)\n",
        "\n",
        "\n",
        "train_loader, test_loader,scaler = create_inout_sequences_pm(data, tw, test_size)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "gruVAE = GRUVAE(6,300,300,1,294,4,5,False)\n",
        "print(gruVAE)\n",
        "vae_trainer = AutoEncoderTrainer(gruVAE, optim.Adam, train_loader, test_loader, validation_size,0.0009)\n",
        "model_list, val_loss_list = vae_trainer.train_and_evaluate(300) \n",
        "\n",
        "\n",
        "val, idx = min((val, idx) for (idx, val) in enumerate(val_loss_list))\n",
        "model = model_list[idx]\n",
        "\n",
        "\n",
        "\n",
        "def inverse(y, scaler):\n",
        "    d = torch.cat([torch.tensor(y),torch.tensor(y),\\\n",
        "                  torch.tensor(y),torch.tensor(y),torch.tensor(y),torch.tensor(y),torch.tensor(y)], dim=-1).numpy()\n",
        "    d = scaler.inverse_transform(d)\n",
        "    return d\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "prev_x = test_loader.dataset.tensors[0][:validation_size][-1].to(\"cuda\")\n",
        "prev_y = test_loader.dataset.tensors[1][:validation_size][-1].to(\"cuda\")\n",
        "\n",
        "def compute_last_h(trials, prev_x, prev_y):\n",
        "    errors = []\n",
        "    h_list = []\n",
        " \n",
        "    metric = nn.MSELoss()\n",
        "    reconstruction_list = []\n",
        "    enc_list = []\n",
        "    for i in range(trials):\n",
        "        _, reconstructions, hidden_states, _, _,_,_,_,enc = model(prev_x, prev_y)\n",
        "        errors.append(metric(torch.stack(reconstructions).view(-1)[-1], prev_y[1:][-1]))\n",
        "        h_list.append(hidden_states)\n",
        "        enc_list.append(enc)\n",
        "        reconstruction_list.append(torch.stack(reconstructions).detach().to(\"cpu\").numpy())\n",
        "    val, idx = min((val, idx) for (idx, val) in enumerate(errors))\n",
        "    return h_list[idx][-1], reconstruction_list[idx],enc_list[idx]\n",
        "\n",
        "last_val_h, reconstructions,enc_list = compute_last_h(500, prev_x, prev_y)\n",
        "\n",
        "# print(last_val_h.shape)\n",
        "pred_size = 30\n",
        "new_x = test_loader.dataset.tensors[0][validation_size:].view(-1,6).to(\"cuda\")\n",
        "new_y = test_loader.dataset.tensors[1][validation_size:].view(-1)\n",
        "new_y = inverse(new_y.view(-1,1),scaler)[:,6]\n",
        "# print(new_x.shape)\n",
        "def make_predictions(model, pred_size, h_start, scaler, new_x,enc_list):\n",
        "    predicted_prices = []\n",
        "    H_t = h_start.to(\"cuda\").unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "     \n",
        "    \n",
        "    for i in range(pred_size):\n",
        "\n",
        "        new_Z = model.Z_prior(h_start)\n",
        "        new_Z = torch.chunk(new_Z, 2, dim=-1)\n",
        "        Z_std = torch.exp(0.5*new_Z[1])\n",
        "        new_Z = sample_from_normal(new_Z[0],Z_std)\n",
        "        x_prime = torch.cat([new_Z, new_x[i].view(-1).float()]).unsqueeze(0).unsqueeze(0)\n",
        "        if model.need_emb == True:\n",
        "            x_prime = model.mpp(x_prime)\n",
        "        tem = model.mha(x_prime,H_t,H_t,None)[0]\n",
        "        tem = model.layernorm(x_prime+tem)\n",
        "        H_t = torch.cat((H_t,tem),1)\n",
        "        h_start = tem[0][0]\n",
        "        _,predicted_price,_ = model.reconstruct(h_start)\n",
        "        predicted_price = predicted_price.to(\"cpu\")\n",
        "        predicted_price = inverse(predicted_price.view(-1,1),scaler)[:,6]\n",
        "        predicted_prices.append(max(predicted_price,0))\n",
        "\n",
        "    \n",
        "    # for i in range(pred_size):\n",
        "        \n",
        "    return predicted_prices\n",
        "    \n",
        "\n",
        "def trial(trials, model, hidden_states, pred_size, new_x,enc_list):\n",
        "    predictions = []\n",
        "    upper_90 = []\n",
        "    lower_90 = []\n",
        "    upper_95 = []\n",
        "    lower_95 = []\n",
        "    mean = []\n",
        "    for i in range(trials):\n",
        "        predicted_prices = make_predictions(model, pred_size, hidden_states, scaler, new_x,enc_list)\n",
        "        predicted_prices = np.vstack(predicted_prices)\n",
        "        predictions.append(predicted_prices)\n",
        "       \n",
        "    predictions = np.concatenate(predictions, axis=1)\n",
        "        \n",
        "    for j in range(pred_size):\n",
        "        temp = predictions[j]\n",
        "        upper_90.append(np.percentile(temp,95))\n",
        "        lower_90.append(np.percentile(temp,5))\n",
        "        upper_95.append(np.percentile(temp,97.5))\n",
        "        lower_95.append(np.percentile(temp,2.5))\n",
        "        mean.append(np.mean(temp))\n",
        "    print(RMSELoss(torch.tensor(mean),new_y[:pred_size]))\n",
        "    return upper_90, lower_90, upper_95, lower_95, mean\n",
        "\n",
        "#\n",
        "\n",
        "\n",
        "u_90, l_90, u_95, l_95, m = trial(1000, model, last_val_h, pred_size, new_x,enc_list)\n",
        "\n",
        "  \n",
        "############################################################################\n",
        "offset = 200\n",
        "plt.rcParams[\"figure.figsize\"] = (15,6)\n",
        "actual_values = data[\"pm2.5\"].tolist()[:tw*(len(data)//tw)]\n",
        "\n",
        "end_idx = val_steps+pred_size+train_steps\n",
        "start_idx = end_idx-pred_size-val_steps\n",
        "actual_range = actual_values[start_idx:end_idx]\n",
        "\n",
        "\n",
        "validation_end = len(actual_range) - pred_size + 1\n",
        "validation_start = validation_end - val_steps\n",
        "min_val = min(actual_range[validation_start:validation_end])\n",
        "max_val = max(actual_range[validation_start:validation_end])\n",
        "\n",
        "\n",
        "plt.plot(actual_range,color=\"teal\", label=\"actual\")\n",
        "plt.plot([i+val_steps for i in range(len(m))],m, color=\"deeppink\", label=\"mean prediction\")\n",
        "plt.fill_between([i+val_steps for i in range(len(m))], u_95, l_95,\n",
        "                 color='deeppink', alpha=0.2, label=\"95% confidence interval\")\n",
        "plt.fill_between([i+val_steps for i in range(len(m))], u_90, l_90,\n",
        "                 color='cyan', alpha=0.2, label=\"90% confidence interval\")\n",
        "\n",
        "plt.xlabel(\"observation point\")\n",
        "plt.ylabel(\"pm 2.5\")\n",
        "plt.legend(loc=\"upper left\",  prop={\"size\":15})\n",
        "plt.grid()\n",
        "ticks,labels = plt.xticks()\n",
        "ticks = np.array(list(ticks)[1:-1]).astype(int)\n",
        "labels = ticks+1201\n",
        "plt.xticks(ticks, labels)\n",
        "\n",
        "#######################################\n",
        "pred_list = [5,10,15,20,25,30,40,50]\n",
        "for i in pred_list:\n",
        "  u_90, l_90, u_95, l_95, m = trial(500, model, last_val_h, i, new_x,None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "InLBTQDNTYVn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}